{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c443b989-5a71-455f-9a59-9963338634ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:8: FutureWarning: Setting `workspace='/home/ubuntu/ahmads/monarch/examples'` is deprecated.\n",
      "torchx.schedulers.slurm_scheduler 2025-08-29 20:40:34 INFO unable to get job info for `monarch-ubuntu` with `squeue` (squeue: error: Invalid job id: monarch-ubuntu\n",
      "), trying `sacct`\n",
      "torchx.schedulers.slurm_scheduler 2025-08-29 20:40:34 INFO unable to get job info for `monarch-ubuntu` with `sacct` (sacct: fatal: Bad job/step specified: monarch-ubuntu\n",
      ")\n",
      "monarch.tools.commands 2025-08-29 20:40:34 INFO no existing RUNNING server `slurm:///monarch-ubuntu` creating new one...\n",
      "torchx.runner.api 2025-08-29 20:40:34 INFO Tracker configurations: {}\n",
      "torchx.runner.api 2025-08-29 20:40:34 INFO Checking for changes in workspace `/home/ubuntu/.monarch/out/tmp3m4zzjg6/workspace`...\n",
      "torchx.runner.api 2025-08-29 20:40:34 INFO To disable workspaces pass: --workspace=\"\" from CLI or workspace=None programmatically.\n",
      "torchx.runner.api 2025-08-29 20:40:34 INFO Reusing original image `monarch_default_workspace:latest` for role[0]=mesh0. Either a patch was built or no changes to workspace was detected.\n",
      "monarch.tools.commands 2025-08-29 20:40:34 INFO created new `slurm:///410` waiting for it to be ready...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahmad: {'requeue': None, 'ntasks-per-node': '1', 'cpus-per-task': '48', 'mem': '186777', 'gpus-per-task': '4', 'ntasks': '1'}\n",
      "Ahmad: {'requeue': None, 'ntasks-per-node': '1', 'cpus-per-task': '48', 'mem': '186777', 'gpus-per-task': '4', 'ntasks': '1'}\n",
      "Waiting for slurm:///410 to be RUNNING (current: PENDING); will check again in 5.0 seconds. Total wait time: 0:00:00.015800\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[-]E0829 20:40:34.996334  8536 hyperactor/src/channel/net.rs:695] error_msg:session tcp:10.0.2.236:26600.5117454862225131082: failed to deliver message within timeout\n",
      "[-]E0829 20:40:35.341902  8536 hyperactor/src/channel/net.rs:708] error_msg:session tcp:10.0.2.132:26600.8381289842876906331: failed to receive ack within timeout 30 secs; link is currently broken\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for slurm:///410 to be RUNNING (current: PENDING); will check again in 5.0 seconds. Total wait time: 0:00:10.059201\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "slurm.utils 2025-08-29 20:40:49 INFO \n",
      "===== Server Info =====\n",
      "{\n",
      "  \"name\": \"410\",\n",
      "  \"server_handle\": \"slurm:///410\",\n",
      "  \"state\": \"RUNNING\",\n",
      "  \"meshes\": {\n",
      "    \"mesh0\": {\n",
      "      \"host_type\": \"__UNSET__\",\n",
      "      \"hosts\": 2,\n",
      "      \"gpus\": -1,\n",
      "      \"hostnames\": [\n",
      "        \"gpu-queue-st-gpu-compute-1\",\n",
      "        \"gpu-queue-st-gpu-compute-2\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "monarch._src.actor.allocator 2025-08-29 20:40:49 INFO no match label `procmesh.monarch.meta.com/name` specified in alloc constraints\n",
      "monarch._src.actor.allocator 2025-08-29 20:40:49 INFO found a single proc mesh `mesh0` in slurm:///410, will allocate on it\n",
      "monarch.tools.network 2025-08-29 20:40:49 INFO no AF_INET6 address that can bind TCP sockets for `gpu-queue-st-gpu-compute-1:26600` (error: [Errno -5] No address associated with hostname)\n",
      "monarch.tools.network 2025-08-29 20:40:49 INFO resolved AF_INET address `10.0.2.236:26600` for `gpu-queue-st-gpu-compute-1:26600`\n",
      "monarch.tools.network 2025-08-29 20:40:49 INFO no AF_INET6 address that can bind TCP sockets for `gpu-queue-st-gpu-compute-2:26600` (error: [Errno -5] No address associated with hostname)\n",
      "monarch.tools.network 2025-08-29 20:40:49 INFO resolved AF_INET address `10.0.2.132:26600` for `gpu-queue-st-gpu-compute-2:26600`\n",
      "monarch._src.actor.allocator 2025-08-29 20:40:49 INFO initializing alloc on remote allocator addresses: ['tcp!10.0.2.236:26600', 'tcp!10.0.2.132:26600']\n",
      "monarch._src.actor.allocator 2025-08-29 20:40:49 INFO no match label `procmesh.monarch.meta.com/name` specified in alloc constraints\n",
      "monarch._src.actor.allocator 2025-08-29 20:40:49 INFO found a single proc mesh `mesh0` in slurm:///410, will allocate on it\n",
      "monarch.tools.network 2025-08-29 20:40:49 INFO no AF_INET6 address that can bind TCP sockets for `gpu-queue-st-gpu-compute-1:26600` (error: [Errno -5] No address associated with hostname)\n",
      "monarch.tools.network 2025-08-29 20:40:49 INFO resolved AF_INET address `10.0.2.236:26600` for `gpu-queue-st-gpu-compute-1:26600`\n",
      "monarch.tools.network 2025-08-29 20:40:49 INFO no AF_INET6 address that can bind TCP sockets for `gpu-queue-st-gpu-compute-2:26600` (error: [Errno -5] No address associated with hostname)\n",
      "monarch.tools.network 2025-08-29 20:40:49 INFO resolved AF_INET address `10.0.2.132:26600` for `gpu-queue-st-gpu-compute-2:26600`\n",
      "monarch._src.actor.allocator 2025-08-29 20:40:49 INFO initializing alloc on remote allocator addresses: ['tcp!10.0.2.236:26600', 'tcp!10.0.2.132:26600']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New job `slurm:///410` is ready to serve.\n",
      "\u001b[36m>>> Aggregated Logs (2025-08-29 20:40:55) >>>\u001b[0m\n",
      "\u001b[33m[8 similar log lines]\u001b[0m self.rank=7 Initializing torch distributed\n",
      "\u001b[33m[8 similar log lines]\u001b[0m [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "\u001b[33m[8 similar log lines]\u001b[0m self.rank=0 Finished initializing torch distributed\n",
      "\u001b[33m[8 similar log lines]\u001b[0m self.rank=0 Running basic DDP example\n",
      "\u001b[33m[8 similar log lines]\u001b[0m self.rank=5 local_rank=1\n",
      "\u001b[36m<<< Aggregated Logs (2025-08-29 20:40:58) <<<\u001b[0m\n",
      "\n",
      "DDP example completed successfully!\n",
      "\u001b[36m>>> Aggregated Logs (2025-08-29 20:40:58) >>>\u001b[0m\n",
      "\u001b[33m[8 similar log lines]\u001b[0m self.rank=6 Finished running basic DDP example\n",
      "\u001b[33m[8 similar log lines]\u001b[0m self.rank=0 Cleaning up torch distributed\n",
      "\u001b[36m<<< Aggregated Logs (2025-08-29 20:41:01) <<<\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[-]E0829 20:41:30.158934  8536 hyperactor/src/channel/net.rs:695] error_msg:session tcp:10.0.2.132:26600.11111315873644166091: failed to deliver message within timeout\n",
      "[-]E0829 20:41:30.774458  8536 hyperactor/src/channel/net.rs:695] error_msg:session tcp:10.0.2.236:26600.6097672994633804723: failed to deliver message within timeout\n",
      "[-]E0829 20:41:34.705394  8536 hyperactor/src/channel/net.rs:695] error_msg:session tcp:10.0.2.236:38955.9004778724387042266: failed to deliver message within timeout\n"
     ]
    }
   ],
   "source": [
    "# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.\n",
    "\n",
    "# @noautodeps\n",
    "# pyre-ignore-all-errors\n",
    "import logging\n",
    "import os\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from monarch.tools import commands\n",
    "from monarch.actor import Actor, current_rank, endpoint\n",
    "from monarch.actor import Actor, current_rank, endpoint\n",
    "from monarch.utils import setup_env_for_distributed\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from slurm.utils import get_appdef, get_server_info, create_proc_mesh\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(name)s %(asctime)s %(levelname)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    force=True,\n",
    ")\n",
    "\n",
    "\n",
    "logger: logging.Logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    \"\"\"A simple toy model for demonstration purposes.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.net1 = nn.Linear(10, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.net2 = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net2(self.relu(self.net1(x)))\n",
    "\n",
    "\n",
    "class DDPActor(Actor):\n",
    "    \"\"\"This Actor wraps the basic functionality from Torch's DDP example.\n",
    "\n",
    "    Conveniently, all of the methods we need are already laid out for us,\n",
    "    so we can just wrap them in the usual Actor endpoint semantic with some\n",
    "    light modifications.\n",
    "\n",
    "    Adapted from: https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html#basic-use-case\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rank = current_rank().rank\n",
    "\n",
    "    def _rprint(self, msg):\n",
    "        \"\"\"Helper method to print with rank information.\"\"\"\n",
    "        print(f\"{self.rank=} {msg}\")\n",
    "\n",
    "    @endpoint\n",
    "    async def setup(self):\n",
    "        \"\"\"Initialize the PyTorch distributed process group.\"\"\"\n",
    "        self._rprint(\"Initializing torch distributed\")\n",
    "\n",
    "        WORLD_SIZE = int(os.environ[\"WORLD_SIZE\"])\n",
    "        # initialize the process group\n",
    "        dist.init_process_group(\"gloo\", rank=self.rank, world_size=WORLD_SIZE)\n",
    "        self._rprint(\"Finished initializing torch distributed\")\n",
    "\n",
    "    @endpoint\n",
    "    async def cleanup(self):\n",
    "        \"\"\"Clean up the PyTorch distributed process group.\"\"\"\n",
    "        self._rprint(\"Cleaning up torch distributed\")\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "    @endpoint\n",
    "    async def demo_basic(self):\n",
    "        \"\"\"Run a basic DDP training example.\"\"\"\n",
    "        self._rprint(\"Running basic DDP example\")\n",
    "\n",
    "        # create model and move it to GPU with id rank\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "        self._rprint(f\"{local_rank=}\")\n",
    "        model = ToyModel().to(local_rank)\n",
    "        ddp_model = DDP(model, device_ids=[local_rank])\n",
    "\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ddp_model(torch.randn(20, 10))\n",
    "        labels = torch.randn(20, 5).to(local_rank)\n",
    "        loss_fn(outputs, labels).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"{self.rank=} Finished running basic DDP example\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    num_hosts = 2\n",
    "    appdef = await get_appdef(num_hosts)\n",
    "    server_info = await get_server_info(appdef)\n",
    "\n",
    "    try:\n",
    "        proc_mesh = await create_proc_mesh(num_hosts, appdef, server_info)\n",
    "\n",
    "        ddp_actor = await proc_mesh.spawn(\"ddp_actor\", DDPActor)\n",
    "\n",
    "        await setup_env_for_distributed(proc_mesh)\n",
    "\n",
    "        await ddp_actor.setup.call()\n",
    "        await ddp_actor.demo_basic.call()\n",
    "        await ddp_actor.cleanup.call()\n",
    "\n",
    "        print(\"DDP example completed successfully!\")\n",
    "\n",
    "    finally:\n",
    "        commands.kill(f\"slurm:///{server_info.name}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7fcfc7-3561-43bd-9945-278fb488e0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahmads-nightly4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
